from __future__ import annotations

from typing import Any, Dict, Iterable, List, Optional
from pymilvus import (
    MilvusClient,
    DataType,
    FieldSchema,
    CollectionSchema,
    Function,
    FunctionType,
)

from sentence_transformers import SentenceTransformer
import numpy as np


# -----------------------------
# Helpers: Ticket -> dict row
# -----------------------------
def ticket_to_dict(ticket: Any) -> Dict[str, Any]:
    if hasattr(ticket, "model_dump"):
        return ticket.model_dump()
    if isinstance(ticket, dict):
        return ticket
    return dict(ticket)


def make_doc_text(ticket_row: Dict[str, Any]) -> str:
    return (
        f"{ticket_row.get('subject','')}\n"
        f"{ticket_row.get('description','')}\n"
        f"{ticket_row.get('error_logs','')}"
    ).strip()


# -----------------------------
# Create collection (dense + BM25 sparse)
# -----------------------------
def create_ticket_collection(
    uri: str = "http://localhost:19530",
    collection_name: str = "support_tickets",
    dense_dim: int = 384,  # all-MiniLM-L6-v2 = 384
    drop_if_exists: bool = False,
) -> MilvusClient:
    client = MilvusClient(uri=uri)

    if client.has_collection(collection_name):
        if drop_if_exists:
            client.drop_collection(collection_name)
        else:
            return client

    fields = [
        FieldSchema(name="ticket_id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),

        # Raw text used for BM25 -> sparse_vec
        FieldSchema(
            name="doc_text",
            dtype=DataType.VARCHAR,
            max_length=8192,
            enable_analyzer=True,                 # important for text processing
            analyzer_params={"type": "english"},  # change to multilingual later if needed
            enable_match=True,                    # enables match/filter on text when needed
        ),

        # BM25 sparse embeddings generated by built-in function
        FieldSchema(name="sparse_vec", dtype=DataType.SPARSE_FLOAT_VECTOR),

        # Dense embeddings you provide
        FieldSchema(name="dense_vec", dtype=DataType.FLOAT_VECTOR, dim=dense_dim),

        # metadata for filtering
        FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="subcategory", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="product", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="product_module", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="resolution_code", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="resolution_helpful", dtype=DataType.BOOL),
    ]

    # Built-in BM25: doc_text -> sparse_vec
    bm25_fn = Function(
        name="bm25_doc_text",
        function_type=FunctionType.BM25,
        input_field_names=["doc_text"],
        output_field_names=["sparse_vec"],
    )

    schema = CollectionSchema(
        fields=fields,
        description="Support tickets for hybrid (dense + BM25 sparse) search",
        functions=[bm25_fn],
    )

    client.create_collection(collection_name=collection_name, schema=schema)

    # Indexes: dense + sparse (BM25 needs an index too)
    index_params = client.prepare_index_params()

    # Dense vector index (AUTO_INDEX is simplest)
    index_params.add_index(
        field_name="dense_vec",
        index_type="AUTO_INDEX",
        metric_type="IP",   # use IP if you normalize embeddings
    )

    # Sparse BM25 index (AUTO_INDEX works; BM25 metric triggers BM25 behavior)
    index_params.add_index(
        field_name="sparse_vec",
        index_type="AUTO_INDEX",
        metric_type="BM25",
    )

    client.create_index(collection_name=collection_name, index_params=index_params)
    client.load_collection(collection_name)

    return client


# -----------------------------
# Insert tickets (Ticket objects / dicts)
# -----------------------------
def insert_tickets(
    client: MilvusClient,
    collection_name: str,
    tickets: Iterable[Any],
    embed_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    batch_size: int = 256,
) -> None:
    ticket_rows = [ticket_to_dict(t) for t in tickets]

    doc_texts = [make_doc_text(r) for r in ticket_rows]

    embedder = SentenceTransformer(embed_model_name)
    dense_vecs = embedder.encode(
        doc_texts,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,  # important if you use IP
        show_progress_bar=True,
    )

    data = []
    for r, doc_text, dense in zip(ticket_rows, doc_texts, dense_vecs):
        data.append({
            "ticket_id": r.get("ticket_id", ""),
            "doc_text": doc_text,
            "dense_vec": dense.tolist(),

            "category": r.get("category", "") or "",
            "subcategory": r.get("subcategory", "") or "",
            "product": r.get("product", "") or "",
            "product_module": r.get("product_module", "") or "",
            "resolution_code": r.get("resolution_code", "") or "",
            "resolution_helpful": bool(r.get("resolution_helpful", False)),
        })

    client.insert(collection_name=collection_name, data=data)


from src.data.ingestion import load_tickets
json_data = load_tickets("/home/ramesh/Personal_projects/AI-based-Product-Support-System/data/raw/support_tickets.json")
Data_tickets = json_data[0]
Data_tickets[:5]
uri="http://localhost:19530"
client = create_ticket_collection(
    uri=uri,
    collection_name="support_tickets",
    dense_dim=384,
    drop_if_exists=True,   # for dev
)

insert_tickets(client, "support_tickets", Data_tickets[:5])  # tickets = [Ticket(...), ...]
